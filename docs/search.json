[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Maria Prokofieva",
    "section": "",
    "text": "ML Engineer | Data Scientist | Academic | Cyberpsychology and Business\nCheck my Google Scholar\n\nEducation\nPhD in Computer Science | Graduate Certificate in Accounting | Master in Computer Science | Bachelor of Computer Science and Linguistics |\n\n\nExperience\nCPA Australia: Fellow CPA Australia | Business Analytics Discussion Group R Consortium: Working Group | RBusiness Senior Lecturer: Victoria University, Australia Lead Data Scientist: Mitchell Institute for Policy Development"
  },
  {
    "objectID": "posts/fitbit_google/index.html",
    "href": "posts/fitbit_google/index.html",
    "title": "Fitbit in your Google Sheets in 5 min or less",
    "section": "",
    "text": "We all want to be healthy (and wealthy!) and wearable devices is a popular answer to track your fitness level or check how close (or .. well.. not that close) to your fitness goals. Fitbit has been quite popular and used more than by 37 million only active users who use their device at least once a week, according to this Statista report.\nso why not take a closer look at it? in your Google Sheets!\nDownloading your data straight to Google Sheets is an awesome idea as you can:\nanalyse your data yourself, especially, if you have a particular fitness goal in mind (e.g. sleep for 8 hours on average, run a marathon each week, you name it!) share your data with someone (e.g. your doctor, your fitness trainer, your kid — YES, show them!) or you are just a control freak and what to make sure it’s YOURS. And Google Sheets makes it as convenient as possible — and as quick as possible, bet you — 5 minutes! (well, it’s telling me it’s 6 min reading time, so don’t count your reading time, pls!)\nSo, let’s dig it — well it is a mild dig, so no rollup sleeves, pls :)\nWhat we need:\nGoogle Account (with Google Sheets)\nFitbit with data and Fitbit account\n… and 5 minutes of your time. The MOST HARD to get from the list!\nThe result:\nGoogle Sheets with your Fitbit data and ability to synchronise it with time"
  },
  {
    "objectID": "posts/fitbit_google/index.html#so-lets-do-it",
    "href": "posts/fitbit_google/index.html#so-lets-do-it",
    "title": "Fitbit in your Google Sheets in 5 min or less",
    "section": "So let’s do it!",
    "text": "So let’s do it!\nWe will need work with your Google Sheets and your Fitbit developer account so have both handy open in two separate screens\nGoogle Sheets side: Go to Google Sheets and create a new file there\n\nGo to Extensions in the menu and click on Apps Script\n\nWe are going to use GoogleFitbit extension by JKybett\nIt’s “approved” by Fitbit and is listed under their page here — you will not get “hacked”\nNow, you need to click on this link and copy all code from here.\n\nand click on the Save icon (little disk there on top!)\nClick on Libraries + on the left hand side to see this pop-up window\n\nand search for THIS id below:\n1B7FSrk5Zi6L1rSxxTDgDEUsPzlukDsi4KGuTMorsTQHhGBzBkMun4iDF\n(if you don't trust me — check the link i provided earlier for the code!)\n\nVersion: select the latest (i.e. the bigger number! which is currently 43).\nIdentifier: keep as is!\nClick Add and close the window\nGo back to your Google Sheets page and REFRESH! this is really important!\nYou will need to wait a few seconds (yeahhhh… veeeeeery looooong) and you will see Fitbit option in the menu\n\nNow, we need to actually set it up! so click on Fitbit and then Setup\n\nIt is most likely going to ask your permission to run the script we setup — so Continue\n\nand you will need to provide your email for Fitbit and probably it is going to warn you again:\n\nand you click on Go to Fitbit (unsafe)\nand then we allow Fitbit to access our Google account, since we are going to pour its data to our Google Sheets\n\nOnce it's done — you refresh your Google Sheets page again and select Fitbit>Setup (again) to see this\n\nWhat you need to from this screen is the Redirect URL which you need to copy and paste (for your reference for now!) to be used 5 sec later when you go to..\n2. Fitbit side:\nGo to Fitbit DEVELOPER website. This is NOT your Fitbit account at https://www.fitbit.com/\nYou need https://dev.fitbit.com/\nGo to Manage>Register An App (see here)\n\nand provide some details, but some of those entries can be what-ever-you-want (blurred on the screenshot), but others — you need to provide specific information (highlighted in RED).\n\nFor those entries with URL at the end — don't stress! you do not need your personal website. Remember, you are doing this for your OWN data!\nApplication Name *: give it a name you like\nDescription *: and description you like\nApplication Website URL *: this needs to start with http:// but then you can include ANYTHING (it may not be the real website for our purposes)\nOrganization *: whatever-you-like\nOrganization Website URL *: start with http:// and then anything (e.g. http://google.com)\nTerms of Service URL *: same as for the above: start with http:// and then anything\nPrivacy Policy URL *: same!\nOAuth 2.0 Application Type *: select Personal\nRedirect URL *: paste the code you were provided in Google Sheets !THIS IS REALLY IMPORTANT!\nDefault Access Type *: select Read & Write\nDon't forget to put a checkbox in \"I have read and agree to the terms of service\"\nAfter registering your app, you can access it and edit under \"Manage my app\" tab.\n\nwhooohoo! we are getting close!\nFrom the app page you need to note two codes:\nOAuth 2.0 Client ID\nClient Secret\nWe will use them in our Google Sheets\n\nSo back there!\nGoogle Sheets (again)\nthe next setup step is to enter the details of the freshly registered Fitbit app here (promise — we are nearly done!)\n\nand hit Submit!\nTHIS. IS. IT!\nYou quickly authorise your app one more time and…\nMAGIC!\nSEE THIS\nyour Google Sheets will be quickly populated with the dates and data you requested!\n\nas you can see — you can\n\nreset it (with your new details)\nsync to get today's data\nor get your Custom dates!\n\nEnjoy! you are the champ!"
  },
  {
    "objectID": "posts/design1/bootstrap1.html",
    "href": "posts/design1/bootstrap1.html",
    "title": "Beautiful Data with Bootstrap and React",
    "section": "",
    "text": "In the world of data visualization, presenting data in an engaging and beautiful way is crucial for effective communication (and your pay). Today we are going to look at how we can combine two “superpowers” of web presentation: Bootstrap and React and we demonstrate it in another two “superpowers” - R and Python\nLets make our data-driven web applications shine!"
  },
  {
    "objectID": "posts/design1/bootstrap1.html#the-power-of-bootstrap-the-king-is-dead-no.-its-immortal",
    "href": "posts/design1/bootstrap1.html#the-power-of-bootstrap-the-king-is-dead-no.-its-immortal",
    "title": "Beautiful Data with Bootstrap and React",
    "section": "1: The Power of Bootstrap: the king is dead? no. it’s immortal",
    "text": "1: The Power of Bootstrap: the king is dead? no. it’s immortal\nWe will be quick here and go straight to the point.\nBootstrap is a widely used and popular front-end framework for web development. It was initially developed by Twitter, but it was pre-Elon Musk and Boostrap is (still! hooray!) is an open-source project.\nIf you need an AWESOME design and you have no time for that (looking at the earlier versions of this website) go to Bootstrap! It is a magic in a box that comes with a collection of pre-built HTML, CSS, and JavaScript components and tools that take seconds to create responsive, visually appealing, and consistent web designs. Yeah, you don’t need a designer degree now to get start and running in minutes with a huge range of design elements (grids, navigation bars, buttons, forms, and … you name it).\nThe selling point behind Bootstrap is its flexibility, customizability, and responsiveness (mind the disambiguation :) ). You can’t really be a data person these days without some-handy-knowledge of how to “sell” your data insights with attractive and user-friendly user design: welcome boostrap into your life!\nIntegrating Bootstrap with your code opens up a world of possibilities where your dashboards or web apps are not only powerful, but are also visually stunning. Think of responsive design components, such as grids, navigation bars, and form elements, can be seamlessly incorporated into your data-heavy projects. A picture is worth a thousand words. An animation gives you a million. An interaction makes you a billionaire! Innteractive data dashboards, reports, or online tools, Bootstrap’s CSS and JavaScript components will make data-crunching efforts look fantastic on screens of all sizes, from desktops to mobile devices."
  },
  {
    "objectID": "posts/design1/bootstrap1.html#how-can-we-bootstrap-in-r-and-python",
    "href": "posts/design1/bootstrap1.html#how-can-we-bootstrap-in-r-and-python",
    "title": "Beautiful Data with Bootstrap and React",
    "section": "2: How can we bootstrap in R and Python",
    "text": "2: How can we bootstrap in R and Python\nWe focus on these two environments: Python and R. Well, you cannot hide it but both are MADE for data! We are not going to fight wars on which one is “the one”, just let’s be … open!!! (and open-source as well :) )\nR Markdown Documents:\nIntegrate Bootstrap styles and components into R Markdown documents to create stylish and responsive HTML reports. You can do this by adding Bootstrap CSS classes to HTML elements within your R Markdown document. Shiny Web Applications:\nWhen developing interactive web applications with Shiny, you can incorporate Bootstrap for better design and responsiveness. Use the “shinydashboard” package, which is built on Bootstrap, to create interactive dashboards. Flexdashboard:\nCreate interactive dashboards with Flexdashboard, an R package that combines R Markdown and Bootstrap. It allows you to build dashboards with responsive layouts and interactive elements. HTML Widgets:\nBuild custom HTML widgets using R packages like “htmlwidgets” and style them with Bootstrap for consistent and visually appealing components. R Web Apps:\nIf you’re developing full-fledged web applications with R, you can use Bootstrap directly in the HTML and CSS of your application. This provides complete control over the design and layout. R Shiny Themes:\nCustomize the appearance of Shiny applications by applying Bootstrap themes. There are R packages available that provide pre-built Bootstrap themes for Shiny. Integrating Bootstrap Libraries:\nLink to Bootstrap libraries in your HTML documents or Shiny applications to leverage the full suite of Bootstrap features. You can include Bootstrap CSS and JavaScript files from content delivery networks (CDNs) or locally. Reticulate:\nIf you’re using R within a Python environment through reticulate, you can still use Bootstrap for front-end development when building web applications using Flask or Django.\nIntegrating Bootstrap with R and Python Discuss how Bootstrap can be seamlessly integrated into web applications created with R and Python. Emphasize the use of Bootstrap CSS and JavaScript components. Section 2: Building Interactive UIs with React What is React? Provide a brief introduction to React, a JavaScript library for building user interfaces. Explain the component-based approach that React follows. Combining React with R and Python Describe how React can be utilized to build interactive user interfaces in R and Python. Highlight the benefits of using React components for data visualization. Section 3: Creating Beautiful Data Applications Responsive Data Dashboards Showcase how Bootstrap’s responsive design makes your data dashboards look great on various devices. Mention the use of Bootstrap’s grid system for creating responsive layouts. Interactive Data Visualizations Explain how React’s component-based structure facilitates the creation of interactive charts and data visualizations. Provide examples of data-driven web applications using React and data visualization libraries like Plotly and D3. Section 4: Case Studies Case Study 1: R + Bootstrap + React Present a real-world example of a data application created using R, Bootstrap, and React. Discuss the data source, visualization techniques, and the technology stack used. Case Study 2: Python + Bootstrap + React Provide another case study involving Python, Bootstrap, and React. Highlight the unique features of the application and its impact on data presentation. Section 5: Conclusion Summarize the key points discussed in the blog post. Encourage readers to explore the powerful combination of Bootstrap and React in R and Python for beautiful data applications. Mention the importance of user-friendly, interactive, and visually appealing data presentations in today’s data-driven world. In this blog post, we’ve explored how combining Bootstrap and React with R and Python can transform your data into beautiful, interactive, and user-friendly applications. By harnessing the power of these technologies, you can take your data visualization skills to the next level, making your data not only informative but also visually captivating."
  },
  {
    "objectID": "posts/conda/index.html",
    "href": "posts/conda/index.html",
    "title": "Python’s Conda Jungle: a no-brainer from Anaconda to Miniforge",
    "section": "",
    "text": "If you are starting with Python you are most likely have a question: should I go straight and just install/use Python or do I need anything else?\nOr likely that you started your journey with Anaconda all-in-one release being promoted as the “World’s Most Popular Data Science Platform” (well, by Anaconda themselves).\nOr you may came across some tutorials that refer to mysterious “conda”. So. Do you need it? Why? Which? Fortunately, not “how much” - its free!\n\n\n\n\n\nConda typically refers to the package and environment manager, while “Anaconda” and “Miniconda” are distribution options. “Package and environment manager” means what it means: “manages your packages” and “manages your environments”.\nLets do the “package manager” part first: you need a package manager to helps you find, install, update, and remove software packages, libraries, or dependencies in a consistent and controlled manner. This ensures that your software projects have access to the required components and that different projects don’t interfere with each other.\nExample:\nWe have a freshly installed Python and we want to do some simple ml wrangling. We need to have scikit-learn package. So we go to trusty terminal and pip\npip install scikit-learn\njust to discover (hopefully with no really big time wasted) that it’s not gonna work and we also need another package,\npip install numpy\nWith conda, you just nail it with one line:\nconda install scikit-learn\nEnjoy!\nNow, the environment management part is that it’s a good practice to work in environment - think of your cosy nook where you know everything, adjust everything (e.g. put all your comfort teddies around, aroma candles :)) and just WORK.\nIn a more computer manner that “cosy nook” is your envirnoment, or “virtual environments”, or “conda environments.” These environments are separate from the system-wide environment and from each other. This isolation helps you keep your projects self-contained and free from possible conflicts, like conflict between different version of python or packages. Think about that as an ultimate manifestation of “work-life” balance “:) put your”life” and “work into separate environments :)\nConda help you create isolated environments for different projects, help you organise your projects. This is particularly valuable in data science, scientific computing, and software development, where complex and interdependent libraries and dependencies are common. You can specify the exact versions of packages and dependencies to use in each environment, making it easier to reproduce your work and share it with others.\nBest of all, once you have your “cosy nook” environment, you can easily export it with conda as well:\nconda env export > environment.yml\nand use it later.\nSo now you are sold on condas, let’s get the right conda for you.\n\n\nAnaconda: The Comprehensive Toolbox\n\n\n\n\n\nWhen to Use Anaconda:\nAnaconda is your go-to choice if you’re seeking an all-in-one data science platform. It bundles together Conda, Python, and an extensive collection of pre-built data science libraries and tools. It’s an ideal solution for data scientists, researchers, and analysts who want a hassle-free, ready-to-use environment for their work.\n\nInstallation:\nDownload Anaconda from the official website and install it.\n\n\nWhy Anaconda?\n\nGenie in a bottle (but big bottle…):\n\nPre-packaged Libraries: Anaconda comes with hundreds of pre-installed libraries, including NumPy, pandas, scikit-learn, Jupyter, and more, saving you the effort of installing them individually.\nEasy Environment Management: Create isolated environments for different projects with specific dependencies, ensuring reproducibility.\nAvailable for Windows, macOS, and Linux.\n\n\nBeware:\nThe gift of Anaconda comes with a big box of “commitments”, a large installation size, eating your resources and coming with lots of other software that you probably never need….\n\n\n\n\nMiniconda: Size does not matter when it comes to strength\n\n\n\n\n\nMiniconda is a minimal Conda installer that allows you to build a customized environment tailored to your specific requirements. It’s the same ginny but in a IKEA packaging :)\n\nWhy Miniconda:\nPerfect for users who want more control over their environment and prefer a lightweight footprint. It goes straight to the minimalist soul :)\n\n\nInstallation:\nDownload Miniconda from the official website and install it. After installation, use Conda to create a new environment and install packages as needed.\n\n\nWhy Miniconda?\nSmall, but mighty: Miniconda is a lean installer, allowing you to install only what you need.\nControl freak yourself!: Build environments with precisely the packages you require, keeping your system free from unnecessary clutter.\nQuicky: fast to install, making it a great choice for rapid environment configuration.\n\n\nBeware:\nLet’s get dirty: You’ll need to set up environments and install packages manually, which may require more initial effort.\n\n\n\n\nConda-Forge: people’s princess\n\n\n\n\n\nConda-Forge is a community-driven collection of Conda packages.\nIt’s a fantastic resource for users who require packages not available in the default Anaconda repository. You can access an extensive range of community-contributed packages through Conda-Forge.\nExample: you have your new shiny Apple Pro macbook and “lucky” enough for it to be on M1 chip. And “lucky” enough to want to run your advanced ML models. Prepare for the trouble.. which we will discuss later :)\n\nInstallation:\nAdd Conda-Forge using this command in your Terminal:\nconda config --add channels conda-forge.\nInstall packages from Conda-Forge as needed.\n\n\nWhy Conda-Forge\nThe world is not enough: provides an extensive selection of packages that are not found in the default Anaconda repository.\nAlone we can do so little, together we can do so much: maintained and contributed to by the Conda community, ensuring up-to-date and well-maintained packages.\n\n\nBeware:\nTrust your sources: while Conda-Forge offers many valuable packages, be aware that they are not officially maintained by Anaconda Inc. Ensure you trust the source of the packages you install.\n\n\n\n\nWhen to Use What: instead of boring summary\nAnaconda:\n\nChoose it for a comprehensive, out-of-the-box data science environment with pre-packaged libraries.\nIdeal for beginners and those who need a complete solution.\n\nMiniconda:\n\nOpt for a lightweight, customizable installation if you want more control over your environment and prefer a minimal footprint.\n\nConda-Forge:\n\nbest of its all (yeah, I am a people’s person!)\nyou have a “buggy” setup\nyou need additional packages beyond what’s available in the default Anaconda repository.\n\n\n\nStay on track!\nIn this world of no-commitments, stay tuned for what’s happening and be ready to evolve!\nLatest and greatest: the latest conda’s news and developments are critical in data science and scientific computing.\nTop security: Regular updates ensure you benefit from security patches and bug fixes, enhancing the stability of your environment.\nPeople’s power: Monitoring the community’s activity around Conda packages can provide insights into the popularity and reliability of specific libraries and tools."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Let’s get out of the ivory tower and get busy."
  },
  {
    "objectID": "posts/LLM0/llm0.html",
    "href": "posts/LLM0/llm0.html",
    "title": "Intro into LLM",
    "section": "",
    "text": "Bootstrap knowledge of LLMs ASAP. With a bias/focus to GPT.\nAvoid being a link dump. Try to provide only valuable well tuned information.\n\n\nNeural network links before starting with transformers.\n\nhttps://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\nhttps://www.3blue1brown.com/topics/neural-networks\nhttp://neuralnetworksanddeeplearning.com/\nhttps://distill.pub/\n\n\n\n\n\n🟢 = easy, 🟠 = medium, 🔴 = hard\n🕰️ = long, 🙉 = low quality audio\n\n\n\n\n\n🟢🕰️ Łukasz Kaiser Attention is all you need; Attentional Neural Network Models This talk is from 6 years ago.\n🟢🕰️ Andrej Karpathy The spelled-out intro to language modeling: building makemore: basic. bi-gram name generator model by counting, then by NN. using pytorch.\n🟢🕰️ Andrej Karpathy Building makemore Part 2: MLP:\n🕰️ Andrej Karpathy Building makemore Part 3: Activations & Gradients, BatchNorm):\n🕰️ Andrej Karpathy Building makemore Part 4: Becoming a Backprop Ninja:\n🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings: Tokens are embedded into a semantic space. sine/cosine position encoding explained very well.\n🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 2) Multi-Head & Self-Attention: Clear overview of multi-head attention.\n🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 3) Decoder’s Masked Attention: Further details on the transformer architecture.\n🟠🕰️ Andrej Karpathy Andrej Karpathy - Let’s build GPT: from scratch, in code, spelled out.: build up a Shakespeare gpt-2-like from scratch. starts with bi-gram and adds features one by one. pytorch.\n🔴🕰️ Chris Olah CS25 I Stanford Seminar - Transformer Circuits, Induction Heads, In-Context Learning: Interpretation. Deep look into the mechanics of induction heads. Companion article\n🟢 Jay Alammar The Illustrated Word2vec - A Gentle Intro to Word Embeddings in Machine Learning\n🟢 Jay Alammar How GPT3 Works - Easily Explained with Animations: extremely high level basic overview.\n🟢🕰️ Jay Alammar The Narrated Transformer Language Model: much deeper look at the architecture. goes into detail. Companion article.\nSebastian Raschka L19: Self-attention and transformer networks Academic style lecture series on self-attention transformers\n🟢🕰️🙉 Mark Chen Transformers in Language: The development of GPT Models including GPT3 A chunk of this lecture is about applying GPT to images. Same lecture series as the Chris Olah one. Rest of the series. Papers listed in the talk:\n\n“GPT-1”: Liu et. al. Generating Wikipedia by Summarizing Long Sequences\n“GPT-2”: Radford et. al. Language Models are Unsupervised Multitask Learners github.com/openai/gpt-2 OpenAI: Better Language Models Fermats Library\n“GPT-3”: Brown et. al. Language Models are Few-Shot Learners (I think this is it, can’t find the quoted text inside this paper)"
  },
  {
    "objectID": "posts/LLM0/LLM.html",
    "href": "posts/LLM0/LLM.html",
    "title": "Purpose",
    "section": "",
    "text": "Bootstrap knowledge of LLMs ASAP. With a bias/focus to GPT.\nAvoid being a link dump. Try to provide only valuable well tuned information.\n\n\nNeural network links before starting with transformers.\n\nhttps://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\nhttps://www.3blue1brown.com/topics/neural-networks\nhttp://neuralnetworksanddeeplearning.com/\nhttps://distill.pub/\n\n\n\n\n\n🟢 = easy, 🟠 = medium, 🔴 = hard\n🕰️ = long, 🙉 = low quality audio\n\n\n\n\n\n🟢🕰️ Łukasz Kaiser Attention is all you need; Attentional Neural Network Models This talk is from 6 years ago.\n🟢🕰️ Andrej Karpathy The spelled-out intro to language modeling: building makemore: basic. bi-gram name generator model by counting, then by NN. using pytorch.\n🟢🕰️ Andrej Karpathy Building makemore Part 2: MLP:\n🕰️ Andrej Karpathy Building makemore Part 3: Activations & Gradients, BatchNorm):\n🕰️ Andrej Karpathy Building makemore Part 4: Becoming a Backprop Ninja:\n🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 1) Position Embeddings: Tokens are embedded into a semantic space. sine/cosine position encoding explained very well.\n🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 2) Multi-Head & Self-Attention: Clear overview of multi-head attention.\n🟢 Hedu AI Visual Guide to Transformer Neural Networks - (Episode 3) Decoder’s Masked Attention: Further details on the transformer architecture.\n🟠🕰️ Andrej Karpathy Andrej Karpathy - Let’s build GPT: from scratch, in code, spelled out.: build up a Shakespeare gpt-2-like from scratch. starts with bi-gram and adds features one by one. pytorch.\n🔴🕰️ Chris Olah CS25 I Stanford Seminar - Transformer Circuits, Induction Heads, In-Context Learning: Interpretation. Deep look into the mechanics of induction heads. Companion article\n🟢 Jay Alammar The Illustrated Word2vec - A Gentle Intro to Word Embeddings in Machine Learning\n🟢 Jay Alammar How GPT3 Works - Easily Explained with Animations: extremely high level basic overview.\n🟢🕰️ Jay Alammar The Narrated Transformer Language Model: much deeper look at the architecture. goes into detail. Companion article.\nSebastian Raschka L19: Self-attention and transformer networks Academic style lecture series on self-attention transformers\n🟢🕰️🙉 Mark Chen Transformers in Language: The development of GPT Models including GPT3 A chunk of this lecture is about applying GPT to images. Same lecture series as the Chris Olah one. Rest of the series. Papers listed in the talk:\n\n“GPT-1”: Liu et. al. Generating Wikipedia by Summarizing Long Sequences\n“GPT-2”: Radford et. al. Language Models are Unsupervised Multitask Learners github.com/openai/gpt-2 OpenAI: Better Language Models Fermats Library\n“GPT-3”: Brown et. al. Language Models are Few-Shot Learners (I think this is it, can’t find the quoted text inside this paper)"
  },
  {
    "objectID": "posts/cash_flows/index.html#cash-flows---bank-reconciliation",
    "href": "posts/cash_flows/index.html#cash-flows---bank-reconciliation",
    "title": "Cash flow - reconciliation",
    "section": "Cash flows - bank reconciliation",
    "text": "Cash flows - bank reconciliation\nI continue with the series for “nerdy” accountants who want to diverge from Excel and master the power and beauty of R automation - and we are looking at one of the most important areas of ANY business! Cash!\nCash management is a really critical issue for both business owners and people like me who are trying not to look at recent interest rates jumps.\nCash management includes cash collection, handling, and usage of cash (spending!). It is essential to have enough cash to cover immediate expenses, fund business growth and have working capital. Or in simple terms, you need to have enough cash to pay for your coffee, cover your morgage repayment and invest in that Tesla Model 3\n\nCash analysis is an important step to assess companies short-term liquidity, evaluate working capital and make decisions about investments.\nToday, we are going to have a look at the step that comes before cash flow visualization. Much much earlier…. Before we are able to put cash flow items on a nice graph, we need to obtain those cash flow items “somehow”.\nAccountants don’t have cash flow data by default, and there is no magic way to get it. Rather, it is necessary to go transaction by transaction, classify items, group them, collate them, and double-check that they actually occurred! We need to make sure that we are not double-charged as well as we are not underpaying or omitting any of our payments and they are all included in the list.\nWe start backwards from this very list and we dig into doing bank reconciliation and in particular, looking at our (business) bank statement. This is indeed a very useful exercise, not only in regards to your business but also for your own expense management.\nFor this post, we will work through a very simple example, just looking at a bank statement and poking around. It is a “personal” bank statement that comes from Kaggle\n\ncf<-read_csv(\"data/bank_st.csv\")\n\ncf%>%head()\n\n# A tibble: 6 × 7\n  Date     Day   Type  Category `Debit Amount` `Credit Amount` `Closing Balance`\n  <chr>    <chr> <chr> <chr>             <dbl>           <dbl>             <dbl>\n1 1/8/2018 Wedn… Debit Shopping          2500                0           174656.\n2 1/8/2018 Wedn… Debit Shopping           324                0           174332.\n3 2/8/2018 Thur… None  None                 0                0           174332.\n4 3/8/2018 Frid… Debit Shopping           404.               0           173928.\n5 4/8/2018 Satu… Debit Shopping           100                0           173828.\n6 4/8/2018 Satu… Debit Shopping          1395                0           172433.\n\n\nThis is a typical bank statement you can view in your bank account where each row is a transaction for a particular reporting period (e.g. month). We do not have the name of the second party for the transactions (e.g. the name of the store or the company that credited/debited the account), but all transactions have been classified - which can be seen under Category.\nThe dataset has Debit Amount, which is when you were charged, and Credit Amount, which is when you were paid. The Closing Balance is a running balance that shows the amount of cash in your account after the transaction. The most important parts of that Closing Balance are the initial and final numbers and they are used to reconcial (= match) balances in your own “books” (accounting books!= accounting records). If those number do not match, we investigate individual closing balances for the transactions to identify where we were overpaid or underpaid.\nLet’s look closer at the data: it is not messy, but not ideal…\nColumn names have blanks and they do not play well in functions, so let’s use clean_names() from janitor package to make them more R friendly\n\ncf<-cf%>%\n  clean_names()\n\ncf%>%head()\n\n# A tibble: 6 × 7\n  date     day       type  category debit_amount credit_amount closing_balance\n  <chr>    <chr>     <chr> <chr>           <dbl>         <dbl>           <dbl>\n1 1/8/2018 Wednesday Debit Shopping        2500              0         174656.\n2 1/8/2018 Wednesday Debit Shopping         324              0         174332.\n3 2/8/2018 Thursday  None  None               0              0         174332.\n4 3/8/2018 Friday    Debit Shopping         404.             0         173928.\n5 4/8/2018 Saturday  Debit Shopping         100              0         173828.\n6 4/8/2018 Saturday  Debit Shopping        1395              0         172433.\n\n\nThat’s better! so now all variables are in small letters and have snake_case!\n\nnames(cf)\n\n[1] \"date\"            \"day\"             \"type\"            \"category\"       \n[5] \"debit_amount\"    \"credit_amount\"   \"closing_balance\"\n\n\nLet’s explore the data and do some simple counting - yes, we love to count!\nFirst, what is the closing balance and how it changes during the month\nBut before we do so, let’s have a close look at the date column. In the first twenty rows you cans see there are a few issues as some dates include single vs double for days and two-digit vs four-digit for year. It is also in a string format.\n\nclass(cf$date)\n\n[1] \"character\"\n\ncf$date[1:20]\n\n [1] \"1/8/2018\"  \"1/8/2018\"  \"2/8/2018\"  \"3/8/2018\"  \"4/8/2018\"  \"4/8/2018\" \n [7] \"4/8/2018\"  \"4/8/2018\"  \"4/8/2018\"  \"5/8/2018\"  \"6/8/2018\"  \"6/8/2018\" \n[13] \"7/8/2018\"  \"8/8/2018\"  \"9/8/2018\"  \"10/8/2018\" \"10/8/2018\" \"11/8/2018\"\n[19] \"11/8/2018\" \"11/8/2018\"\n\n\nTo fix this, let’s convert to the date type and fix the formating with lubridate package\n\ncf$date<-dmy(cf$date)\n\nNow, let’s see the spend per each billing date. We exclude the days with no spend:\n\ncf%>%\n  group_by(date)%>%\n  summarise(spend=sum(debit_amount))%>%\n  filter(spend!=0)%>%\n  ggplot(aes(date, spend))+\n  geom_line()\n\n\n\n\nNow, let’s see type of categories we have\n\ncf%>%count(category, sort=TRUE)\n\n# A tibble: 10 × 2\n   category          n\n   <chr>         <int>\n 1 Shopping         46\n 2 None             21\n 3 ATM               9\n 4 Interest          8\n 5 Entertainment     7\n 6 Medical           5\n 7 Travel            4\n 8 Restaurant        3\n 9 Rent              2\n10 Salary            2\n\n\nThis None category does not look right…. What is it there…\n\ncf%>% filter(category==\"None\")%>%\n  head()\n\n# A tibble: 6 × 7\n  date       day       type  category debit_amount credit_amount closing_balance\n  <date>     <chr>     <chr> <chr>           <dbl>         <dbl>           <dbl>\n1 2018-08-02 Thursday  None  None                0             0         174332.\n2 2018-08-05 Sunday    None  None                0             0         162098.\n3 2018-08-08 Wednesday None  None                0             0         158597.\n4 2018-08-21 Tuesday   None  None                0             0          91343.\n5 2018-08-24 Friday    None  None                0             0          61755.\n6 2018-08-26 Sunday    None  None                0             0          38441.\n\n\nIt looks like the majority of these entries are not really transactions, but a closing balance. Do we need to include them? Probably not. Let’s confirm that they are not transactions and have debit_amount and credit_amount as zero\n\ncf%>% filter(category==\"None\")%>%\n  filter(debit_amount!=0 | credit_amount!=0)\n\n# A tibble: 0 × 7\n# ℹ 7 variables: date <date>, day <chr>, type <chr>, category <chr>,\n#   debit_amount <dbl>, credit_amount <dbl>, closing_balance <dbl>\n\n\nand it is a good idea to exclude them\n\ncf<-cf%>%filter(category!=\"None\")\n\nLet’s see which day has the most number of transactions and which category is the most used one (what is the money drainer!):\n\ncf%>%count(day, sort=TRUE)\n\n# A tibble: 7 × 2\n  day           n\n  <chr>     <int>\n1 Saturday     36\n2 Friday       11\n3 Thursday     10\n4 Sunday        9\n5 Wednesday     8\n6 Monday        7\n7 Tuesday       5\n\ncf%>%count(category, sort=TRUE)\n\n# A tibble: 9 × 2\n  category          n\n  <chr>         <int>\n1 Shopping         46\n2 ATM               9\n3 Interest          8\n4 Entertainment     7\n5 Medical           5\n6 Travel            4\n7 Restaurant        3\n8 Rent              2\n9 Salary            2\n\n\nWell, good, but does not look nice.. So let’s “paint it”. (We look at spending where credited amount is $0 per category.)\n\n\n\n\nplot4<-cf%>%filter(credit_amount==0)%>%\n  group_by(day)%>%\n  summarise(day_spend=sum(debit_amount),\n            n=n())%>%\n  ggplot(aes(x=fct_reorder(day, desc(day_spend)),\n             y=day_spend))+\n  geom_col()+ \n  labs(x = \"Days\", y = \"$ value\",\ntitle =\"Cash across days\")+\n  theme(\n  panel.border = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  axis.line = element_line(colour = \"black\"),\n  axis.text.x = element_text(angle = 90),\nplot.title = element_textbox(hjust = 0.5,\n                                 width = unit(0.5, \"npc\"),\n                                 margin = margin(b = 15))  )\n\n(plot1|plot2)/(plot3|plot4)\n\n\n\n\nFor a real business, this amount of Saturday transactions would raise a red flag, but - this data is from personal records, so looks like someone is having a blast off after a busy week :)\nAlso, with category that None does not sound right…. it is the second highest so… I would really investigate what sort of None is that None…\nWell, what are out total earn and which days we are paid and what for?\n\ncf%>%filter(credit_amount>0)%>%\n  count(category)\n\n# A tibble: 2 × 2\n  category     n\n  <chr>    <int>\n1 Interest     8\n2 Salary       2\n\n\nIt looks like we have only two major category - interest and salary. Let’s see what brings more money\n\ncf%>%filter(credit_amount>0)%>%\n  group_by(category)%>%\n  summarise(category_total=sum(credit_amount))\n\n# A tibble: 2 × 2\n  category category_total\n  <chr>             <dbl>\n1 Interest          4050.\n2 Salary          500508 \n\n\nWell, it is still salary! but would be sooo good if it is our passive income that drives the cash flows!\nLet’s see the balance for the month…\n\nbalance<-sum(cf$credit_amount)-sum(cf$debit_amount)\n\nbalance\n\n[1] 268715.5\n\n\nWoohoo! Our balance is positive, so we managed to grow our wealth!\nIndeed, it is a very simple example, but a good foundation to start your R experience in accounting! …."
  },
  {
    "objectID": "posts/cash_flows/index.html#references",
    "href": "posts/cash_flows/index.html#references",
    "title": "Cash flow - reconciliation",
    "section": "References",
    "text": "References\nhttps://www.kaggle.com/datasets/sandhaya4u/august-bank-statement-sandhaya"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "What’s new",
    "section": "",
    "text": "Purpose\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPython’s Conda Jungle: a no-brainer from Anaconda to Miniforge\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nFitbit in your Google Sheets in 5 min or less\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nCash flow - reconciliation\n\n\n\n\n\n\n\nbusiness\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nWelcome\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro into LLM\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeautiful Data with Bootstrap and React\n\n\n\n\n\n\n\ndesign\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CV",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Untitled",
    "section": "",
    "text": "Fruit prices {.striped .hover}\n\n\nfruit\nprice\n\n\n\n\napple\n2.05\n\n\npear\n1.37\n\n\norange\n3.09"
  },
  {
    "objectID": "index_b.html",
    "href": "index_b.html",
    "title": "MyDataRules (MyDR)",
    "section": "",
    "text": "Welcome\nI am a data scientist with R and Python under my belt. I love both data wrangling and ML. What else an ML engineer can say.\nBy coincidence I am an ML engineer and academic, so this website is my personal project where I share some of my leisure projects and some snippets of my work project\nI work with clients across the whole domain of data science, starting with\n\nautomated data collection - data wrangling - data storytelling\nto model building and implementation\n\nI am Fellow CPA Australia member and promote the use of data and data tools for business purposes.\nTalk to me if you need advice or guidance and\nOPEN FOR HIRE"
  }
]