[
  {
    "objectID": "posts/inflation/inflation.html",
    "href": "posts/inflation/inflation.html",
    "title": "Understanding inflation in Australia with a help of ABS and SDMX",
    "section": "",
    "text": "Money. Money. Money.\nI hope you have already visualised your 10 mln check (not BILL) coming to your bank account, but let’s be specific and look at the data behind decisions to keep or raise interest rate.\nSo, this week we are going to look at Consumer Price Index, Australia available at the Australian Bureau of Statistics (ABS) website\nThe Consumer Price Index (CPI) measures household inflation and includes statistics about price change for categories of household expenditure.\n\n\n\nPCI at ABS\n\n\n\nData\nYou can either download the data from here\nor use API and get data straight to R using rsdmx package.\nNote: this is NOT your traditional json format, this is RSDMX format. You can read more about RSDMX on its official website. Briefly, SDMX is a standardized framework used to exchange statistical data and metadata. It aims to improve the efficiency and interoperability of data exchange processes.\nBefore using the package make sure that you run:\n\nremotes::install_github(\"opensdmx/rsdmx\")\n\nand then load libraries:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(rsdmx)\n\nand now let’s get the content\n\nmyUrl <- \"https://api.data.abs.gov.au/data/ABS,CPI,1.1.0/1+2+3+4+5.132305+132304+132306+102675+102676+104101+104102+104104+104105+104120+104122+115902+115922+115941+115961+117101+117104+117107+117121+117124+117127+117144+128058+128061+131197+131199+114120+115492+115522+115528+131179+131180+131181+131182+131184+131186+131187+131188+131189+131191+131193+131195+30001+30002+30003+30007+30012+30016+30022+30024+30025+30026+30027+30033+40106+97556+97561+97563+97565+114121+114122+1144+115484+115485+115495+115496+115497+115498+115500+115501+115520+115524+115529+131178+131183+131185+131190+131192+131194+30014+40001+40002+40004+40005+40006+40007+40008+40009+40010+40012+40014+40015+40025+40026+40027+40029+40030+40034+40045+40046+40047+40048+40053+40055+40058+40060+40066+40067+40072+40073+40077+40078+40080+40081+40083+40084+40085+40086+40087+40088+40089+40090+40091+40092+40093+40094+40095+40096+40098+40101+40102+97549+97550+97551+97554+97555+97557+97558+97559+97560+97564+97567+97571+97572+97573+97574+10001+20001+20002+20003+20004+20005+20006+115486+115488+115489+115493+126670+999901+999902+999903.10+20.1+2+3+4+5+6+7+8+50.Q?startPeriod=2000-Q1&dimensionAtObservation=AllDimensions\"\nmyUrl <- \"https://api.data.abs.gov.au/data/ABS,CPI,1.1.0/1+2+3+4+5.10001+20001+20002+20003+20004+20005+20006+115486+115488+115489+115493+126670+999901+999902+999903.10+20.50.Q?startPeriod=2000-Q1&dimensionAtObservation=AllDimensions\"\n\ndataset <- as.data.frame(readSDMX(myUrl))\n\nLet’s have a snippet of our data:\n\n\nRows: 440820 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): TIME_PERIOD, FREQ, UNIT_MEASURE, OBS_STATUS\ndbl (6): MEASURE, INDEX, TSEST, REGION, obsValue, DECIMALS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAnd we have the following names in the dataset and first 20 rows there\n\nnames(dataset)\n\n [1] \"TIME_PERIOD\"  \"MEASURE\"      \"INDEX\"        \"TSEST\"        \"REGION\"      \n [6] \"FREQ\"         \"obsValue\"     \"UNIT_MEASURE\" \"DECIMALS\"     \"OBS_STATUS\"  \n\ndataset|>head(20)\n\n# A tibble: 20 × 10\n   TIME_PERIOD MEASURE  INDEX TSEST REGION FREQ  obsValue UNIT_MEASURE DECIMALS\n   <chr>         <dbl>  <dbl> <dbl>  <dbl> <chr>    <dbl> <chr>           <dbl>\n 1 2000-Q1           3 115941    10      1 Q          3   PCT                 1\n 2 2000-Q2           3 115941    10      1 Q          3.5 PCT                 1\n 3 2000-Q3           3 115941    10      1 Q          6   PCT                 1\n 4 2000-Q4           3 115941    10      1 Q          5.8 PCT                 1\n 5 2001-Q1           3 115941    10      1 Q          6.5 PCT                 1\n 6 2001-Q2           3 115941    10      1 Q          6.3 PCT                 1\n 7 2001-Q3           3 115941    10      1 Q          3   PCT                 1\n 8 2001-Q4           3 115941    10      1 Q          3.6 PCT                 1\n 9 2002-Q1           3 115941    10      1 Q          3   PCT                 1\n10 2002-Q2           3 115941    10      1 Q          2.8 PCT                 1\n11 2002-Q3           3 115941    10      1 Q          3.2 PCT                 1\n12 2002-Q4           3 115941    10      1 Q          2.8 PCT                 1\n13 2003-Q1           3 115941    10      1 Q          3.3 PCT                 1\n14 2003-Q2           3 115941    10      1 Q          2.5 PCT                 1\n15 2003-Q3           3 115941    10      1 Q          2.1 PCT                 1\n16 2003-Q4           3 115941    10      1 Q          2.4 PCT                 1\n17 2004-Q1           3 115941    10      1 Q          2   PCT                 1\n18 2004-Q2           3 115941    10      1 Q          2.5 PCT                 1\n19 2004-Q3           3 115941    10      1 Q          2.8 PCT                 1\n20 2004-Q4           3 115941    10      1 Q          2.9 PCT                 1\n# ℹ 1 more variable: OBS_STATUS <chr>\n\n\nNow, what are those variables and what are values there? We need to get metadata and we will use a very convenient ABS sdmx data dictionary to get it.\nFirst, we need to get the Data Structure Definition (DSD) data, using the link from ABS website.\nIn rsdmx the properties of objects in DSD are calleds slots and can be accessed using the slot method. We extract the information about the individual slots in the DSD document we obtained already using slot() and get a list of codelists using sapply().\nNote: dds is a S4 class and that is why we are using sapply()\n\ndds<- readSDMX(\"https://api.data.abs.gov.au/dataflow/ABS/CPI/1.1.0?references=all\")\n\ncodelist_data<-slot(dds, \"codelists\")\n\nWe have the following codelists in our DSD:\n\n\n\nPCI at ABS\n\n\n\nconcepts <- as.data.frame(slot(dsd, \"concepts\"))\n\n#get concepts from DSD concepts <- as.data.frame(slot(dsd, “concepts”))\n`\n\nstats|>summary()"
  },
  {
    "objectID": "posts/cash_flows/index.html#cash-flows---bank-reconciliation",
    "href": "posts/cash_flows/index.html#cash-flows---bank-reconciliation",
    "title": "Cash flow - reconciliation",
    "section": "Cash flows - bank reconciliation",
    "text": "Cash flows - bank reconciliation\nI continue with the series for “nerdy” accountants who want to diverge from Excel and master the power and beauty of R automation - and we are looking at one of the most important areas of ANY business! Cash!\nCash management is a really critical issue for both business owners and people like me who are trying not to look at recent interest rates jumps.\nCash management includes cash collection, handling, and usage of cash (spending!). It is essential to have enough cash to cover immediate expenses, fund business growth and have working capital. Or in simple terms, you need to have enough cash to pay for your coffee, cover your morgage repayment and invest in that Tesla Model 3\n\nCash analysis is an important step to assess companies short-term liquidity, evaluate working capital and make decisions about investments.\nToday, we are going to have a look at the step that comes before cash flow visualization. Much much earlier…. Before we are able to put cash flow items on a nice graph, we need to obtain those cash flow items “somehow”.\nAccountants don’t have cash flow data by default, and there is no magic way to get it. Rather, it is necessary to go transaction by transaction, classify items, group them, collate them, and double-check that they actually occurred! We need to make sure that we are not double-charged as well as we are not underpaying or omitting any of our payments and they are all included in the list.\nWe start backwards from this very list and we dig into doing bank reconciliation and in particular, looking at our (business) bank statement. This is indeed a very useful exercise, not only in regards to your business but also for your own expense management.\nFor this post, we will work through a very simple example, just looking at a bank statement and poking around. It is a “personal” bank statement that comes from Kaggle\n\ncf<-read_csv(\"data/bank_st.csv\")\n\ncf%>%head()\n\n# A tibble: 6 × 7\n  Date     Day   Type  Category `Debit Amount` `Credit Amount` `Closing Balance`\n  <chr>    <chr> <chr> <chr>             <dbl>           <dbl>             <dbl>\n1 1/8/2018 Wedn… Debit Shopping          2500                0           174656.\n2 1/8/2018 Wedn… Debit Shopping           324                0           174332.\n3 2/8/2018 Thur… None  None                 0                0           174332.\n4 3/8/2018 Frid… Debit Shopping           404.               0           173928.\n5 4/8/2018 Satu… Debit Shopping           100                0           173828.\n6 4/8/2018 Satu… Debit Shopping          1395                0           172433.\n\n\nThis is a typical bank statement you can view in your bank account where each row is a transaction for a particular reporting period (e.g. month). We do not have the name of the second party for the transactions (e.g. the name of the store or the company that credited/debited the account), but all transactions have been classified - which can be seen under Category.\nThe dataset has Debit Amount, which is when you were charged, and Credit Amount, which is when you were paid. The Closing Balance is a running balance that shows the amount of cash in your account after the transaction. The most important parts of that Closing Balance are the initial and final numbers and they are used to reconcial (= match) balances in your own “books” (accounting books!= accounting records). If those number do not match, we investigate individual closing balances for the transactions to identify where we were overpaid or underpaid.\nLet’s look closer at the data: it is not messy, but not ideal…\nColumn names have blanks and they do not play well in functions, so let’s use clean_names() from janitor package to make them more R friendly\n\ncf<-cf%>%\n  clean_names()\n\ncf%>%head()\n\n# A tibble: 6 × 7\n  date     day       type  category debit_amount credit_amount closing_balance\n  <chr>    <chr>     <chr> <chr>           <dbl>         <dbl>           <dbl>\n1 1/8/2018 Wednesday Debit Shopping        2500              0         174656.\n2 1/8/2018 Wednesday Debit Shopping         324              0         174332.\n3 2/8/2018 Thursday  None  None               0              0         174332.\n4 3/8/2018 Friday    Debit Shopping         404.             0         173928.\n5 4/8/2018 Saturday  Debit Shopping         100              0         173828.\n6 4/8/2018 Saturday  Debit Shopping        1395              0         172433.\n\n\nThat’s better! so now all variables are in small letters and have snake_case!\n\nnames(cf)\n\n[1] \"date\"            \"day\"             \"type\"            \"category\"       \n[5] \"debit_amount\"    \"credit_amount\"   \"closing_balance\"\n\n\nLet’s explore the data and do some simple counting - yes, we love to count!\nFirst, what is the closing balance and how it changes during the month\nBut before we do so, let’s have a close look at the date column. In the first twenty rows you cans see there are a few issues as some dates include single vs double for days and two-digit vs four-digit for year. It is also in a string format.\n\nclass(cf$date)\n\n[1] \"character\"\n\ncf$date[1:20]\n\n [1] \"1/8/2018\"  \"1/8/2018\"  \"2/8/2018\"  \"3/8/2018\"  \"4/8/2018\"  \"4/8/2018\" \n [7] \"4/8/2018\"  \"4/8/2018\"  \"4/8/2018\"  \"5/8/2018\"  \"6/8/2018\"  \"6/8/2018\" \n[13] \"7/8/2018\"  \"8/8/2018\"  \"9/8/2018\"  \"10/8/2018\" \"10/8/2018\" \"11/8/2018\"\n[19] \"11/8/2018\" \"11/8/2018\"\n\n\nTo fix this, let’s convert to the date type and fix the formating with lubridate package\n\ncf$date<-dmy(cf$date)\n\nNow, let’s see the spend per each billing date. We exclude the days with no spend:\n\ncf%>%\n  group_by(date)%>%\n  summarise(spend=sum(debit_amount))%>%\n  filter(spend!=0)%>%\n  ggplot(aes(date, spend))+\n  geom_line()\n\n\n\n\nNow, let’s see type of categories we have\n\ncf%>%count(category, sort=TRUE)\n\n# A tibble: 10 × 2\n   category          n\n   <chr>         <int>\n 1 Shopping         46\n 2 None             21\n 3 ATM               9\n 4 Interest          8\n 5 Entertainment     7\n 6 Medical           5\n 7 Travel            4\n 8 Restaurant        3\n 9 Rent              2\n10 Salary            2\n\n\nThis None category does not look right…. What is it there…\n\ncf%>% filter(category==\"None\")%>%\n  head()\n\n# A tibble: 6 × 7\n  date       day       type  category debit_amount credit_amount closing_balance\n  <date>     <chr>     <chr> <chr>           <dbl>         <dbl>           <dbl>\n1 2018-08-02 Thursday  None  None                0             0         174332.\n2 2018-08-05 Sunday    None  None                0             0         162098.\n3 2018-08-08 Wednesday None  None                0             0         158597.\n4 2018-08-21 Tuesday   None  None                0             0          91343.\n5 2018-08-24 Friday    None  None                0             0          61755.\n6 2018-08-26 Sunday    None  None                0             0          38441.\n\n\nIt looks like the majority of these entries are not really transactions, but a closing balance. Do we need to include them? Probably not. Let’s confirm that they are not transactions and have debit_amount and credit_amount as zero\n\ncf%>% filter(category==\"None\")%>%\n  filter(debit_amount!=0 | credit_amount!=0)\n\n# A tibble: 0 × 7\n# ℹ 7 variables: date <date>, day <chr>, type <chr>, category <chr>,\n#   debit_amount <dbl>, credit_amount <dbl>, closing_balance <dbl>\n\n\nand it is a good idea to exclude them\n\ncf<-cf%>%filter(category!=\"None\")\n\nLet’s see which day has the most number of transactions and which category is the most used one (what is the money drainer!):\n\ncf%>%count(day, sort=TRUE)\n\n# A tibble: 7 × 2\n  day           n\n  <chr>     <int>\n1 Saturday     36\n2 Friday       11\n3 Thursday     10\n4 Sunday        9\n5 Wednesday     8\n6 Monday        7\n7 Tuesday       5\n\ncf%>%count(category, sort=TRUE)\n\n# A tibble: 9 × 2\n  category          n\n  <chr>         <int>\n1 Shopping         46\n2 ATM               9\n3 Interest          8\n4 Entertainment     7\n5 Medical           5\n6 Travel            4\n7 Restaurant        3\n8 Rent              2\n9 Salary            2\n\n\nWell, good, but does not look nice.. So let’s “paint it”. (We look at spending where credited amount is $0 per category.)\n\n\n\n\nplot4<-cf%>%filter(credit_amount==0)%>%\n  group_by(day)%>%\n  summarise(day_spend=sum(debit_amount),\n            n=n())%>%\n  ggplot(aes(x=fct_reorder(day, desc(day_spend)),\n             y=day_spend))+\n  geom_col()+ \n  labs(x = \"Days\", y = \"$ value\",\ntitle =\"Cash across days\")+\n  theme(\n  panel.border = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.grid.minor = element_blank(),\n  axis.line = element_line(colour = \"black\"),\n  axis.text.x = element_text(angle = 90),\nplot.title = element_textbox(hjust = 0.5,\n                                 width = unit(0.5, \"npc\"),\n                                 margin = margin(b = 15))  )\n\n(plot1|plot2)/(plot3|plot4)\n\n\n\n\nFor a real business, this amount of Saturday transactions would raise a red flag, but - this data is from personal records, so looks like someone is having a blast off after a busy week :)\nAlso, with category that None does not sound right…. it is the second highest so… I would really investigate what sort of None is that None…\nWell, what are out total earn and which days we are paid and what for?\n\ncf%>%filter(credit_amount>0)%>%\n  count(category)\n\n# A tibble: 2 × 2\n  category     n\n  <chr>    <int>\n1 Interest     8\n2 Salary       2\n\n\nIt looks like we have only two major category - interest and salary. Let’s see what brings more money\n\ncf%>%filter(credit_amount>0)%>%\n  group_by(category)%>%\n  summarise(category_total=sum(credit_amount))\n\n# A tibble: 2 × 2\n  category category_total\n  <chr>             <dbl>\n1 Interest          4050.\n2 Salary          500508 \n\n\nWell, it is still salary! but would be sooo good if it is our passive income that drives the cash flows!\nLet’s see the balance for the month…\n\nbalance<-sum(cf$credit_amount)-sum(cf$debit_amount)\n\nbalance\n\n[1] 268715.5\n\n\nWoohoo! Our balance is positive, so we managed to grow our wealth!\nIndeed, it is a very simple example, but a good foundation to start your R experience in accounting! …."
  },
  {
    "objectID": "posts/cash_flows/index.html#references",
    "href": "posts/cash_flows/index.html#references",
    "title": "Cash flow - reconciliation",
    "section": "References",
    "text": "References\nhttps://www.kaggle.com/datasets/sandhaya4u/august-bank-statement-sandhaya"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CV",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Untitled",
    "section": "",
    "text": "Fruit prices {.striped .hover}\n\n\nfruit\nprice\n\n\n\n\napple\n2.05\n\n\npear\n1.37\n\n\norange\n3.09"
  },
  {
    "objectID": "index_b.html",
    "href": "index_b.html",
    "title": "What’s new",
    "section": "",
    "text": "Purpose\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPython’s Conda Jungle: a no-brainer from Anaconda to Miniforge\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeautiful Data with Bootstrap and React\n\n\n\n\n\n\n\ndesign\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntro into LLM\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding inflation in Australia with a help of ABS and SDMX\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nFitbit in your Google Sheets in 5 min or less\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nCash flow - reconciliation\n\n\n\n\n\n\n\nbusiness\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\n  \n\n\n\n\nWelcome\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nMaria Prokofieva\n\n\n\n\n\n\nNo matching items"
  }
]